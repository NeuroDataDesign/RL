{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "colab": {
      "name": "Q_Learning_frozenLake.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A7fa1WMOMwC"
      },
      "source": [
        "# Frozen Lake\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPAWi834Wic3"
      },
      "source": [
        "#### import all requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GutUwuMaOMwE"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODCLU5bEOMwF"
      },
      "source": [
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__tL16A-OMwQ"
      },
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZH5jNHOWrpG"
      },
      "source": [
        "#### register 4x4 environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy5GLvJLOMwG"
      },
      "source": [
        "from gym.envs.registration import register\n",
        "register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        ")\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCJLBfV9OMwG",
        "outputId": "3f3772b4-2b2f-4fea-94ea-0bd7312ea610"
      },
      "source": [
        "\n",
        "env = gym.make('FrozenLakeNotSlippery-v0')\n",
        "\n",
        "# Instantiate the Environment.\n",
        "# env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# To check all environments present in OpenAI\n",
        "# print(envs.registry.all())\n",
        "\n",
        "# visualize 4x4 frozen lake\n",
        "env.render()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKqlHWnqOMwH",
        "outputId": "9457542a-32d9-4142-e2aa-9bb92e47871a"
      },
      "source": [
        "# Total number of States and Actions\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "n_rows = int(np.sqrt(n_states))\n",
        "n_cols = int(np.sqrt(n_states))\n",
        "print( \"States = \", n_states)\n",
        "print( \"Actions = \", n_actions)\n",
        "print('Rows = ', n_rows)\n",
        "print('Cols = ', n_cols)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "States =  16\nActions =  4\nRows =  4\nCols =  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3dhzIe7OMwQ"
      },
      "source": [
        "# Start with 80% random actions to explore the environment\n",
        "# exploration decays with each timestep by a factor of 0.999 until it hits 10% exploration rate \n",
        "\n",
        "epsilon = 0.8\n",
        "epsilon_final = 0.1\n",
        "epsilon_decay = 0.999"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK5pin0tOMwQ"
      },
      "source": [
        "gamma = 0.90 # discount factor\n",
        "learning_rate = 0.9 #how important is the difference between q-val from q-table and what's observed"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ldMGZY_hRwH"
      },
      "source": [
        "#### Initialize Q-table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og5nXDVfOMwR",
        "outputId": "99ef41a7-ec06-406b-c7be-ba9314feb016"
      },
      "source": [
        "\n",
        "#Q = np.random.rand(15,4)*(1/100)\n",
        "#Q = np.vstack([Q, np.zeros((1,4))])\n",
        "\n",
        "Q = np.zeros((n_states,n_actions))\n",
        "Q.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE2O7eUHsRmz",
        "outputId": "8e01606c-98d1-49ea-b78d-0a51e3ceed01"
      },
      "source": [
        "Q"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5chvADGMW7FQ"
      },
      "source": [
        "##assign index to each state using state-matrix\n",
        "\n",
        "state_matrix = np.arange(0,16).reshape(4,4)\n",
        "state_matrix\n",
        "\n",
        "def rowsandcols(state):\n",
        "  ''' input: state returned by env\n",
        "      output: location of state as (row,col) tuple'''\n",
        "  return int(np.where(state_matrix ==state)[0]), int(np.where(state_matrix ==state)[1])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96TgiQE9_cmE"
      },
      "source": [
        "def choose_random_action(state): \n",
        "\n",
        "    ''' input:\n",
        "      state = state returned by env.\n",
        "      output = a random action depending on \n",
        "      whether the state is an edge/ corner or landlocked tile'''\n",
        "    k = np.arange(1,n_rows)\n",
        "   ##corner points    \n",
        "    if rowsandcols(state)==( 0,0):\n",
        "      action = random.choice([1,2])\n",
        "    elif rowsandcols(state) ==(0,n_cols -1):\n",
        "      action = random.choice([0,1])\n",
        "    elif rowsandcols(state)==(n_rows -1 , 0):\n",
        "      action = random.choice([2,3])\n",
        "    elif rowsandcols(state)==(n_rows -1 , n_cols -1 ):\n",
        "      action = random.choice([0,3])\n",
        "    ## no action needed to be chosen for (3,3) -- terminal state \n",
        "\n",
        "    ##edge tiles\n",
        "    elif rowsandcols(state)[0] == 0 and rowsandcols(state)[1] in k:\n",
        "      action = random.choice([0,1,2])\n",
        "    elif rowsandcols(state)[0] in k and rowsandcols(state)[1]  == 0:\n",
        "      action = random.choice([1,2,3])\n",
        "    elif rowsandcols(state)[0] in k and rowsandcols(state)[1] == n_cols -1:\n",
        "      action = random.choice([0,1,3])\n",
        "    elif rowsandcols(state)[0] == n_rows -1 or rowsandcols(state)[1] in k :\n",
        "      action = random.choice([0,2,3])\n",
        "  \n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "    \n",
        "    return action"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DZ1MYUEdTk6"
      },
      "source": [
        "def choose_max_action(state, Q):\n",
        "    ''' input:\n",
        "      state = state returned by env.\n",
        "      Q = current Q-table\n",
        "      output = argmax of max q-val for a state '''\n",
        "\n",
        "   ##corner points    \n",
        "    if state == 0:\n",
        "      max_val = np.max(Q[state][1:3])\n",
        "      \n",
        "    elif state== 3:\n",
        "      max_val = np.max(Q[state][0:2])\n",
        "\n",
        "    elif state == 12:\n",
        "      max_val = np.max(Q[state][2,3])\n",
        "    ## no action needed to be chosen for (3,3) -- terminal state \n",
        "\n",
        "    ##edge tiles\n",
        "    elif rowsandcols(state)  == (0,1) or rowsandcols(state)  ==  (0,2):\n",
        "      max_val = np.max(Q[state][0:3])\n",
        "    elif rowsandcols(state) == (1,0) or rowsandcols(state)  == (2,0):\n",
        "      max_val = np.max(Q[state][1:])\n",
        "    elif rowsandcols(state) == (1,3) or rowsandcols(state) == (2,3):\n",
        "      max_val = np.max(Q[state][[0,1,3]])\n",
        "    elif rowsandcols(state) == (3,1) or rowsandcols(state) == (3,2) :\n",
        "      max_val = np.max(Q[state][[0,2,3]])\n",
        "  \n",
        "    else:\n",
        "      max_val = np.max(Q[state])\n",
        "      \n",
        "    action = np.where(Q[state]==max_val)[0][0]\n",
        "    \n",
        "    return int(action)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRwjtruSmRtJ",
        "outputId": "ddadb0f0-587e-4e3a-af8b-91ee3e97ebd7"
      },
      "source": [
        "#arr = np.arange(0,64).reshape(16,4)\n",
        "#np.where(Q[13]==np.max(Q[13]))[0][0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOatlcLRMyfM"
      },
      "source": [
        "num_episodes = 1000\n",
        "steps_total = [] # store number of steps taken in each episode\n",
        "rewards_total = [] #store reward obtained for each episode\n",
        "epsilon_total = [] #store epsilon obtained at the end of each episode"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBD20yPcOMwR"
      },
      "source": [
        "## q learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKW0FxCaOMwR",
        "outputId": "5310f27a-46da-4842-80bc-64841800dc2a"
      },
      "source": [
        "for i_episode in range(num_episodes):\n",
        "    \n",
        "    # resets the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        step += 1\n",
        "        \n",
        "        \n",
        "        ## generating a random num\n",
        "        random_for_epsilon = np.random.rand()\n",
        "        \n",
        "        ## if random num lesser or equal to epsilon, then select random action\n",
        "        if random_for_epsilon <= epsilon:     \n",
        "            action = choose_random_action(state)  ##replace with action = env.action_space.sample() for action without restrictions\n",
        "        \n",
        "        ## else update q-vals for given state with infitismly small random nums and select argmax(a) for Q(s,a)\n",
        "        else: \n",
        "            random_values = Q[state] + np.random.rand(1,n_actions)/100\n",
        "            action =  np.argmax(random_values)\n",
        "        \n",
        "        \n",
        "        ## as epsilon decays with more timesteps, the prob. of selecting a random val < e decays --> more likely to exploit.     \n",
        "        if epsilon > epsilon_final:\n",
        "            epsilon *= epsilon_decay\n",
        "        \n",
        "        ## env gives reward and next state and whether we've reached terminal state upon taking a at current state.. \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        ##if you want reward penalized at for each timestep\n",
        "        ## reward= reward*(0.9**step) \n",
        "\n",
        "        # filling the Q Table - \n",
        "        Q[state, action] = (1- learning_rate)*Q[state, action] + learning_rate*(reward + gamma * np.max(Q[new_state]))\n",
        "        \n",
        "        # Setting new state for next action\n",
        "        state = new_state\n",
        "        \n",
        "        # env.render()\n",
        "        \n",
        "        if done:\n",
        "            steps_total.append(step)\n",
        "            rewards_total.append(reward)\n",
        "            epsilon_total.append(epsilon)\n",
        "            if i_episode % 10 == 0:\n",
        "                print('Episode: {} Reward: {} Steps Taken: {}'.format(i_episode,reward, step, epsilon))\n",
        "            break\n",
        "        \n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 Reward: 0.0 Steps Taken: 5\n",
            "Episode: 10 Reward: 0.0 Steps Taken: 7\n",
            "Episode: 20 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 30 Reward: 0.0 Steps Taken: 4\n",
            "Episode: 40 Reward: 0.0 Steps Taken: 5\n",
            "Episode: 50 Reward: 0.0 Steps Taken: 11\n",
            "Episode: 60 Reward: 0.0 Steps Taken: 4\n",
            "Episode: 70 Reward: 0.0 Steps Taken: 10\n",
            "Episode: 80 Reward: 0.0 Steps Taken: 8\n",
            "Episode: 90 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 100 Reward: 0.0 Steps Taken: 4\n",
            "Episode: 110 Reward: 0.0 Steps Taken: 4\n",
            "Episode: 120 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 130 Reward: 0.0 Steps Taken: 3\n",
            "Episode: 140 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 150 Reward: 0.0 Steps Taken: 8\n",
            "Episode: 160 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 170 Reward: 0.0 Steps Taken: 7\n",
            "Episode: 180 Reward: 0.0 Steps Taken: 8\n",
            "Episode: 190 Reward: 0.0 Steps Taken: 6\n",
            "Episode: 200 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 210 Reward: 0.0 Steps Taken: 6\n",
            "Episode: 220 Reward: 0.0 Steps Taken: 3\n",
            "Episode: 230 Reward: 0.0 Steps Taken: 6\n",
            "Episode: 240 Reward: 0.0 Steps Taken: 23\n",
            "Episode: 250 Reward: 0.0 Steps Taken: 6\n",
            "Episode: 260 Reward: 0.0 Steps Taken: 9\n",
            "Episode: 270 Reward: 1.0 Steps Taken: 8\n",
            "Episode: 280 Reward: 0.0 Steps Taken: 9\n",
            "Episode: 290 Reward: 0.0 Steps Taken: 6\n",
            "Episode: 300 Reward: 0.0 Steps Taken: 4\n",
            "Episode: 310 Reward: 0.0 Steps Taken: 13\n",
            "Episode: 320 Reward: 0.0 Steps Taken: 5\n",
            "Episode: 330 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 340 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 350 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 360 Reward: 0.0 Steps Taken: 5\n",
            "Episode: 370 Reward: 1.0 Steps Taken: 8\n",
            "Episode: 380 Reward: 1.0 Steps Taken: 10\n",
            "Episode: 390 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 400 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 410 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 420 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 430 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 440 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 450 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 460 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 470 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 480 Reward: 1.0 Steps Taken: 8\n",
            "Episode: 490 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 500 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 510 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 520 Reward: 0.0 Steps Taken: 4\n",
            "Episode: 530 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 540 Reward: 1.0 Steps Taken: 10\n",
            "Episode: 550 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 560 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 570 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 580 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 590 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 600 Reward: 1.0 Steps Taken: 12\n",
            "Episode: 610 Reward: 1.0 Steps Taken: 8\n",
            "Episode: 620 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 630 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 640 Reward: 1.0 Steps Taken: 8\n",
            "Episode: 650 Reward: 0.0 Steps Taken: 7\n",
            "Episode: 660 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 670 Reward: 1.0 Steps Taken: 10\n",
            "Episode: 680 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 690 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 700 Reward: 0.0 Steps Taken: 3\n",
            "Episode: 710 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 720 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 730 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 740 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 750 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 760 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 770 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 780 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 790 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 800 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 810 Reward: 1.0 Steps Taken: 8\n",
            "Episode: 820 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 830 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 840 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 850 Reward: 1.0 Steps Taken: 10\n",
            "Episode: 860 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 870 Reward: 1.0 Steps Taken: 8\n",
            "Episode: 880 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 890 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 900 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 910 Reward: 1.0 Steps Taken: 10\n",
            "Episode: 920 Reward: 0.0 Steps Taken: 4\n",
            "Episode: 930 Reward: 1.0 Steps Taken: 8\n",
            "Episode: 940 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 950 Reward: 1.0 Steps Taken: 6\n",
            "Episode: 960 Reward: 0.0 Steps Taken: 4\n",
            "Episode: 970 Reward: 0.0 Steps Taken: 2\n",
            "Episode: 980 Reward: 1.0 Steps Taken: 12\n",
            "Episode: 990 Reward: 1.0 Steps Taken: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwwgWCvKOMwS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "p-LiaalROMwS",
        "outputId": "a007a0c2-76bb-45da-9192-9c26e09f5d5c"
      },
      "source": [
        "plt.axhline(y=6, color='green', linestyle='-')\n",
        "plt.bar(np.arange(0, 1000), steps_total, \n",
        "        width = 0.4, color = 'blue')\n",
        "plt.xlabel('episode index')\n",
        "plt.ylabel('number of steps taken')\n",
        "plt.title('4x4: restricted actions')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "aB_VRjXaYRh8",
        "outputId": "c86ae90b-d678-4724-bc0f-3b21b9de35df"
      },
      "source": [
        "\n",
        "plt.scatter(np.arange(0, 1000), rewards_total, s = 0.5, color = 'blue')\n",
        "plt.title('4x4 rewards for restricted actions')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('1 if success, 0 if failure')\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '1 if success, 0 if failure')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phaICIVYyVsF"
      },
      "source": [
        "##to convert rewards to 1 or 0 when there is a time decay \n",
        "##new_rewards_total = [ 1 if i > 0.0 else 0 for i in rewards_total  ]\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IezG0HJnOMwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202a6a54-c066-4eec-81bb-a0cf42a0ff7e"
      },
      "source": [
        "print(Q)\n",
        "        \n",
        "print(\"Percent of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
        "print(\"Percent of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
        "\n",
        "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
        "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.59049    0.4782969  0.        ]\n",
            " [0.531441   0.         0.42887448 0.        ]\n",
            " [0.47824907 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.6561     0.         0.531441  ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.80983954 0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.729      0.59049   ]\n",
            " [0.6561     0.81       0.81       0.        ]\n",
            " [0.729      0.9        0.         0.72494575]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.9        0.729     ]\n",
            " [0.81       0.         1.         0.81      ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Percent of episodes finished successfully: 0.808\n",
            "Percent of episodes finished successfully (last 100 episodes): 0.91\n",
            "Average number of steps: 6.21\n",
            "Average number of steps (last 100 episodes): 6.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfS1wsDiZD8F"
      },
      "source": [
        "##convert Q-table to pandas dataframe to export to excel"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v6uj1xBqls3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}